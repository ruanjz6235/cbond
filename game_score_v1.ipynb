{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_dates(df_pivot_old, dates, calendar='df'):\n",
    "    df_pivot = df_pivot_old.copy()\n",
    "    if calendar == 'month':\n",
    "        # 填充到每个月的数据(按季度填充)\n",
    "        df_pivot_other = pd.DataFrame(columns=df_pivot.columns, index=dates.difference(df_pivot.index))\n",
    "        df_pivot = pd.concat([df_pivot, df_pivot_other]).sort_index()\n",
    "        # 按季度填充\n",
    "        df_pivot['calendar'] = df_pivot.index.map(lambda x: str(x)[:5] + str((x.month + 2) // 3))\n",
    "        df_pivot['calendar'] = df_pivot['calendar'].shift(-1).ffill()\n",
    "        df_pivot[df_pivot.columns[:-1]] = df_pivot.groupby('calendar').ffill()\n",
    "    elif calendar == 'day':\n",
    "        df_pivot = df_pivot_old.copy()\n",
    "        df_pivot_other = pd.DataFrame(columns=df_pivot.columns, index=dates.difference(df_pivot.index))\n",
    "        df_pivot = pd.concat([df_pivot, df_pivot_other]).sort_index()\n",
    "\n",
    "        df_pivot['calendar'] = df_pivot.index.map(lambda x: str(x)[:5] + str((x.month + 2) // 3))\n",
    "        df_pivot['calendar'] = df_pivot['calendar'].shift(-1).ffill()\n",
    "        df_pivot[df_pivot.columns[:-1]] = df_pivot.groupby('calendar').ffill()\n",
    "    else:\n",
    "        # 不按quarter填充日期，但收益率按照quarter统计\n",
    "        df_pivot['calendar'] = range(len(df_pivot))\n",
    "        df_pivot_other = pd.DataFrame(columns=df_pivot.columns, index=dates.difference(df_pivot.index))\n",
    "        df_pivot = pd.concat([df_pivot, df_pivot_other]).sort_index()\n",
    "        df_pivot['calendar'] = df_pivot['calendar'].ffill()\n",
    "        df_pivot[df_pivot.columns[:-1]] = df_pivot.groupby('calendar').ffill()\n",
    "    return df_pivot[df_pivot.columns[:-1]].dropna(how='all')\n",
    "\n",
    "\n",
    "def get_port_ts_old(score, score_num):\n",
    "    port_ts_tmp = score.where((score > score_num) & (score <= score_num + 1), 0)\n",
    "    port_ts = port_ts_tmp.where(port_ts_tmp == 0, 1).shift(1)\n",
    "    port_count = port_ts.sum(axis=1).rename('num_' + str(score_num + 1))\n",
    "    port_ts = (port_ts.T / port_count).T\n",
    "    return port_ts, port_count\n",
    "\n",
    "\n",
    "def get_backtest2_old(score, price_df, price_df_index, path_name):\n",
    "    cols = score.columns.intersection(price_df.columns)\n",
    "    price_df_new = price_df[cols]\n",
    "    score_new = score[cols]\n",
    "\n",
    "    ret_df = price_df_new.pct_change()\n",
    "    ret_df_index = price_df_index.pct_change()\n",
    "    ret_port = pd.DataFrame(index=score.index)\n",
    "    for score_num in range(5):\n",
    "        port_ts, port_count = get_port_ts_old(score_new, score_num)\n",
    "        port_ts.reset_index().to_feather(f'./{path_name}/{score_num}_port_ts.feather')\n",
    "        ret_attr = (port_ts * ret_df).sum(axis=1).rename('score_' + str(score_num + 1))\n",
    "        ret_port = pd.concat([ret_port, pd.DataFrame(ret_attr), pd.DataFrame(port_count)], axis=1)\n",
    "    score_cols = ret_port.columns[ret_port.columns.str.startswith('score_')]\n",
    "    num_cols = ret_port.columns[ret_port.columns.str.startswith('num_')]\n",
    "    ret_port = pd.concat([ret_port, ret_df_index], axis=1)\n",
    "    ret_port['excess_300'] = ret_port['score_5'] - ret_port['000300.SH']\n",
    "    ret_port['excess_500'] = ret_port['score_5'] - ret_port['000905.SH']\n",
    "    ret_port['ls'] = ret_port['score_5'] - ret_port['000905.SH']\n",
    "    cols2 = ['excess_300', 'excess_500', 'ls']\n",
    "    # ret_port[score_cols] = (ret_port[score_cols] + 1).cumprod() - 1\n",
    "    ret_port = ret_port[(ret_port.index >= '2017-01-01') & (ret_port.index <= '2023-08-17')]\n",
    "    ret_port = ret_port.loc[price_df.index.intersection(ret_port.index)]\n",
    "    ret_port[list(num_cols) + list(score_cols) + list(ret_df_index.columns) + cols2].to_csv(\n",
    "        f'./{path_name}backtest2_detail.csv')\n",
    "\n",
    "\n",
    "def get_port_ts(score, score_percent_last, score_percent_next, score_num):\n",
    "    port_ts_tmp = score.where((score.T > score.quantile(score_percent_last, axis=1)).T & (score.T <= score.quantile(score_percent_next, axis=1)).T, 0)\n",
    "    port_ts = port_ts_tmp.where(port_ts_tmp == 0, 1).shift(1)\n",
    "    port_count = port_ts.sum(axis=1).rename('num_' + str(score_num + 1))\n",
    "    port_ts = (port_ts.T / port_count).T\n",
    "    return port_ts, port_count\n",
    "\n",
    "\n",
    "def get_backtest2(score, price_df, price_df_index, path_name):\n",
    "    cols = score.columns.intersection(price_df.columns).intersection(price_df.columns)\n",
    "    price_df_new = price_df[cols]\n",
    "    score_new = score[cols]\n",
    "\n",
    "    ret_df = price_df_new.pct_change()\n",
    "    ret_df_index = price_df_index.pct_change()\n",
    "    ret_port = pd.DataFrame(index=score.index)\n",
    "    score_percent_lst = [0, 0.1, 0.25, 0.45, 0.7, 1]\n",
    "    # score_percent_lst = [0, 0.02, 0.32, 0.7, 0.95, 1]\n",
    "    for score_num in range(5):\n",
    "        score_percent_last, score_percent_next = score_percent_lst[score_num], score_percent_lst[score_num+1]\n",
    "        port_ts, port_count = get_port_ts(score_new, score_percent_last, score_percent_next, score_num)\n",
    "        port_ts.reset_index().to_feather(f'./{path_name}/{score_num}_port_ts.feather')\n",
    "        ret_attr = (port_ts * ret_df).sum(axis=1).rename('score_' + str(score_num + 1))\n",
    "        ret_port = pd.concat([ret_port, pd.DataFrame(ret_attr), pd.DataFrame(port_count)], axis=1)\n",
    "    score_cols = ret_port.columns[ret_port.columns.str.startswith('score_')]\n",
    "    num_cols = ret_port.columns[ret_port.columns.str.startswith('num_')]\n",
    "    ret_port = pd.concat([ret_port, ret_df_index], axis=1)\n",
    "    ret_port['excess_300'] = ret_port['score_5'] - ret_port['000300.SH']\n",
    "    ret_port['excess_500'] = ret_port['score_5'] - ret_port['000905.SH']\n",
    "    ret_port['ls'] = ret_port['score_5'] - ret_port['000905.SH']\n",
    "    cols2 = ['excess_300', 'excess_500', 'ls']\n",
    "    # ret_port[score_cols] = (ret_port[score_cols] + 1).cumprod() - 1\n",
    "    ret_port = ret_port[(ret_port.index >= '2017-01-01') & (ret_port.index <= '2023-08-17')]\n",
    "    ret_port = ret_port.loc[price_df.index.intersection(ret_port.index)]\n",
    "    ret_port[list(num_cols) + list(score_cols) + list(ret_df_index.columns) + cols2].to_csv(\n",
    "        f'./{path_name}/backtest2_detail.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_left_years_cross_percent\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "all_daily_cbond_new = pd.read_feather('BondDailyData.feather')\n",
    "bond_info = pd.read_feather(r'BondInfo.feather')\n",
    "bond_info['cbond_info_pub_startdate']=pd.to_datetime(bond_info['cbond_info_pub_startdate'])\n",
    "bond_info['cbond_info_pub_enddate']=pd.to_datetime(bond_info['cbond_info_pub_enddate'])\n",
    "bond_info=bond_info.dropna(subset=['cbond_info_pub_startdate']).reset_index(drop=True)\n",
    "\n",
    "price_interest = pd.read_feather(r'price_interest.feather').rename(columns={'date': 'datetime'})\n",
    "all_daily_cbond_new['bond_code'] = all_daily_cbond_new['symbol'].str[:-3]\n",
    "all_daily_cbond_new = all_daily_cbond_new.merge(price_interest[['bond_code', 'datetime', 'resale_price', 'interest']], on=['bond_code', 'datetime'], how='left')\n",
    "all_daily_cbond_new['resale_price'] = all_daily_cbond_new['resale_price'].fillna(100)\n",
    "all_daily_cbond_new['interest'] = all_daily_cbond_new['interest'].fillna(0)\n",
    "all_daily_cbond_new['resale'] = all_daily_cbond_new['resale_price'] + all_daily_cbond_new['interest']\n",
    "\n",
    "bond_info_new = bond_info.copy()\n",
    "bond_info_new.loc[~bond_info_new['cbond_info_transaction_enddate'].isna(), 'cbond_info_pub_enddate'] = bond_info_new['cbond_info_transaction_enddate']\n",
    "bond_info_new['cbond_info_pub_enddate'] = pd.to_datetime(bond_info_new['cbond_info_pub_enddate'])\n",
    "\n",
    "all_daily_cbond_new = all_daily_cbond_new.merge(bond_info_new[['cbond_info_symbol', 'cbond_info_display_name', 'cbond_info_stock_symbol', 'cbond_info_pub_enddate']], left_on='symbol', right_on='cbond_info_symbol', how='inner')\n",
    "all_daily_cbond_new['left_years'] = (pd.DatetimeIndex(all_daily_cbond_new['cbond_info_pub_enddate']) - pd.DatetimeIndex(all_daily_cbond_new['datetime'])).days/365\n",
    "all_daily_cbond_new['convert_divide_paper'] = all_daily_cbond_new['stock_close'] / all_daily_cbond_new['convert_price'] - 1\n",
    "all_daily_cbond_new['convert_divide_paper_new'] = all_daily_cbond_new['stock_close'] / all_daily_cbond_new['convert_price'] * 100 / all_daily_cbond_new['resale'] - 1\n",
    "all_daily_cbond_new = all_daily_cbond_new[all_daily_cbond_new['left_years'] >= 0]\n",
    "# all_daily_cbond_new = all_daily_cbond_new[['datetime', 'symbol', 'stock_symbol', 'cbond_info_display_name', 'convert_premium', 'convert_divide_paper', 'left_years', 'stock_close', 'close']]\n",
    "\n",
    "\n",
    "# %%\n",
    "inds = ['convert_premium', 'convert_divide_paper', 'convert_divide_paper_new', 'left_years']\n",
    "inds_cross_rank = [ind+'_cross_rank' for ind in inds]\n",
    "inds_cross_count = [ind+'_cross_count' for ind in inds]\n",
    "inds_cross_percent = [ind+'_cross_percent' for ind in inds]\n",
    "\n",
    "inds_ts_rank = [ind+'_ts_rank' for ind in inds]\n",
    "inds_ts_count = [ind+'_ts_count' for ind in inds]\n",
    "inds_ts_percent = [ind+'_ts_percent' for ind in inds]\n",
    "\n",
    "inds_rank = [ind+'_rank' for ind in inds]\n",
    "inds_count = [ind+'_count' for ind in inds]\n",
    "inds_percent = [ind+'_percent' for ind in inds]\n",
    "\n",
    "for i in range(len(inds)):\n",
    "    ind, ind_cross_rank, ind_cross_count, ind_cross_percent = inds[i], inds_cross_rank[i], inds_cross_count[i], inds_cross_percent[i]\n",
    "    ind_ts_rank, ind_ts_count, ind_ts_percent = inds_ts_rank[i], inds_ts_count[i], inds_ts_percent[i]\n",
    "    ind_rank, ind_count, ind_percent = inds_rank[i], inds_count[i], inds_percent[i]\n",
    "    all_daily_cbond_new = all_daily_cbond_new.sort_values(['datetime', ind])\n",
    "    all_daily_cbond_new[ind_cross_rank] = all_daily_cbond_new.groupby('datetime')[ind].rank().rename(ind_cross_rank)\n",
    "    all_daily_cbond_new = all_daily_cbond_new.merge(all_daily_cbond_new.groupby('datetime')[ind].count().rename(ind_cross_count), on=['datetime'], how='inner')\n",
    "    all_daily_cbond_new[ind_cross_percent] = all_daily_cbond_new[ind_cross_rank] / all_daily_cbond_new[ind_cross_count]\n",
    "\n",
    "    all_daily_cbond_new = all_daily_cbond_new.sort_values(['symbol', 'datetime']).reset_index(drop=True)\n",
    "    all_daily_cbond_new[ind_ts_percent] = all_daily_cbond_new.groupby('symbol')[ind].rolling(252, min_periods=1).rank(pct=True).rename(ind_ts_percent).reset_index(drop=True)\n",
    "\n",
    "    all_daily_cbond_new[ind_percent] = all_daily_cbond_new[[ind_cross_percent, ind_ts_percent]].mean(axis=1)\n",
    "# 0-1加速度变大\n",
    "# all_daily_cbond_new['left_years_cross_percent'] = np.log2(all_daily_cbond_new['left_years_cross_percent'] + 1)\n",
    "m = 80\n",
    "all_daily_cbond_new['left_years_cross_percent'] = np.log(m * all_daily_cbond_new['left_years_cross_percent'] + 1) / np.log(m+1)\n",
    "# all_daily_cbond_new['left_years_cross_percent'] = np.log(2 * all_daily_cbond_new['left_years_cross_percent'] + 1) / np.log(3)\n",
    "# 0-1加速度不变\n",
    "# all_daily_cbond_new['left_years_cross_percent'] = 1 - (all_daily_cbond_new['left_years_cross_percent'] - 1) ** 2\n",
    "# all_daily_cbond_new['left_years_cross_percent'] = (3 * all_daily_cbond_new['left_years_cross_percent'] - all_daily_cbond_new['left_years_cross_percent'] ** 3) / 2\n",
    "# all_daily_cbond_new['left_years_cross_percent'] = np.sqrt(2 * all_daily_cbond_new['left_years_cross_percent'] - all_daily_cbond_new['left_years_cross_percent'] ** 2)\n",
    "# 0-1加速度变小\n",
    "# all_daily_cbond_new['left_years_cross_percent'] = 2 - 2 ** (1 - all_daily_cbond_new['left_years_cross_percent'])\n",
    "all_daily_cbond = all_daily_cbond_new.copy()\n",
    "\n",
    "\n",
    "# %%\n",
    "path_name = 'left_years_cross_percent'\n",
    "\n",
    "# all_daily_cbond['game_score'] = (1 - all_daily_cbond['convert_premium_percent']) * 2.5 + (1 - all_daily_cbond['convert_divide_paper_percent']) * 1.25 + all_daily_cbond['left_years_cross_percent'] * 1.25\n",
    "all_daily_cbond['game_score'] = (1 - all_daily_cbond['convert_premium_percent']) * 10 / 3 + (1 - all_daily_cbond['left_years_cross_percent']) * 5 / 3\n",
    "all_daily_cbond['convert_premium_percent'] = (1 - all_daily_cbond['convert_premium_percent']) * 5\n",
    "all_daily_cbond['convert_premium_cross_percent'] = (1 - all_daily_cbond['convert_premium_cross_percent']) * 5\n",
    "all_daily_cbond['convert_premium_ts_percent'] = (1 - all_daily_cbond['convert_premium_ts_percent']) * 5\n",
    "all_daily_cbond['convert_divide_paper_percent'] = (1 - all_daily_cbond['convert_divide_paper_percent']) * 5\n",
    "all_daily_cbond['convert_divide_paper_cross_percent'] = (1 - all_daily_cbond['convert_divide_paper_cross_percent']) * 5\n",
    "all_daily_cbond['convert_divide_paper_ts_percent'] = (1 - all_daily_cbond['convert_divide_paper_ts_percent']) * 5\n",
    "\n",
    "all_daily_cbond['left_years_cross_percent'] = (1 - all_daily_cbond['left_years_cross_percent']) * 5\n",
    "score = all_daily_cbond[['datetime', 'symbol', path_name]]\n",
    "score.columns = ['date', 'stock_code', 'score']\n",
    "close = all_daily_cbond[['datetime', 'symbol', 'close']]\n",
    "close.columns = ['date', 'stock_code', 'close']\n",
    "\n",
    "\n",
    "factor_df3 = score.set_index(['date', 'stock_code'])['score'].unstack()\n",
    "factor_df3.index = pd.to_datetime(factor_df3.index)\n",
    "dates = pd.read_feather('tradingdays.feather').rename(columns={'tradingday': 0})\n",
    "dates1 = dates.loc[dates.index % 21 == 1]\n",
    "dates1 = pd.DatetimeIndex(dates1[0])\n",
    "dates2 = dates.loc[dates.index % 5 == 2]\n",
    "dates2 = pd.DatetimeIndex(dates2[0])\n",
    "dates3 = dates.loc[dates.index % 5 == 3]\n",
    "dates3 = pd.DatetimeIndex(dates3[0])\n",
    "dates4 = dates.loc[dates.index % 5 == 4]\n",
    "dates4 = pd.DatetimeIndex(dates4[0])\n",
    "dates5 = dates.loc[dates.index % 5 == 0]\n",
    "dates5 = pd.DatetimeIndex(dates5[0])\n",
    "dates = pd.DatetimeIndex(dates[0])\n",
    "\n",
    "\n",
    "price_df_index = pd.read_feather(r'price_df_index.feather').set_index('index')\n",
    "# path_name = 'game_score'\n",
    "print(f'start_{path_name}')\n",
    "import os\n",
    "if not os.path.exists(f'./{path_name}/'):\n",
    "    os.mkdir(f'./{path_name}/')\n",
    "\n",
    "price_df = close.set_index(['date', 'stock_code'])['close'].unstack()\n",
    "# price_df.columns = price_df.columns.map(dict(zip(price_df.columns, pd.Series(price_df.columns).str[:-3])))\n",
    "path_name_new = path_name + '/1/'\n",
    "if not os.path.exists(f'./{path_name_new}'):\n",
    "    os.mkdir(f'./{path_name_new}')\n",
    "score_new = normalize_dates(factor_df3.loc[dates1.intersection(factor_df3.index).tolist()], dates)\n",
    "get_backtest2_old(score_new, price_df, price_df_index, path_name_new)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "###"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
